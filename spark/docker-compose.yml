version: "3"
volumes:
  hadoop_namenode:
  hadoop_datanode:
services:
  alpine:
    build: ../alpine
    image: dez/base-alpine:latest
    container_name: alpine-hive
    tty: true
  hadoop:
    build: ../hadoop
    image: dez/base-alpine-hadoop:3.3.1
    container_name: hadoop-for-spark
    entrypoint:  ["/opt/hadoop-3.3.1/etc/hadoop/entrypoint.sh"]
    command: ["tail -f /dev/null"]
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - hadoop_namenode:/hadoop/dfs/name
    ports:
      - "50070:50070"
      - "50075:50075"
      - "9870:9870"
      - "9864:9864"
    tty: true
  spark:
    build: .
    image: dez/base-alpine-spark:2.4.5
    container_name: spark
    entrypoint:  ["/entrypoint.sh"]
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    ports:
      - 8080:8080
      - 18080:18080
      - 4040:4040
      - 7077:7077
      - 8081:8081
      - 8082:8081
    depends_on:
      - hadoop
    tty: true
  jupyterlab:
    build:
      context: .
      dockerfile: jupyterlab.Dockerfile
    image: dez/base-alpine-jupyterlab:latest
    container_name: jupyterlab
    ports:
      - 8888:8888
    depends_on:
      - spark
    tty: true

#  spark-worker-1:
#    image: dez/base-alpine-spark:2.4.5
#    container_name: spark-worker-1
#    environment:
#      - SPARK_WORKER_CORES=1
#      - SPARK_WORKER_MEMORY=512m
#    command: ["/usr/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 >> /usr/spark"]
#    ports:
#      - 8081:8081
#    depends_on:
#      - spark-master
#  spark-worker-2:
#    image: dez/base-alpine-spark:2.4.5
#    container_name: spark-worker-2
#    environment:
#      - SPARK_WORKER_CORES=1
#      - SPARK_WORKER_MEMORY=512m
#    ports:
#      - 8082:8081
#    command: ["/usr/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 >> /usr/spark"]
#    depends_on:
#      - spark-master